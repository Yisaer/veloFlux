[manager]
url = "http://127.0.0.1:8080"
timeout_secs = 10

[llm]
# OpenAI-compatible base URL. For OpenAI public API, use:
#   https://api.openai.com/v1
# For DeepSeek, the docs use:
#   https://api.deepseek.com/beta
# The client does NOT auto-add /v1; include the correct prefix in base_url.
base_url = "https://api.deepseek.com/beta"
timeout_secs = 120

# Prefer reading secrets from environment variables.
api_key_env = "DEEPSEEK_API_KEY"

# Chat-completions model name.
model = "deepseek-reasoner"

# Optional per-stage models (defaults to `model` if omitted).
# - `router_model`: used for intent routing (fast/cheap is fine).
# - `preview_model`: used for streaming SQL preview (fast model recommended).
# - `draft_model`: used for structured SQL repair (reasoning model recommended).
router_model = "deepseek-chat"
preview_model = "deepseek-chat"
draft_model = "deepseek-reasoner"

# Whether to stream the *SQL preview* via SSE (improves perceived responsiveness).
# JSON repair steps are executed as non-streaming requests for stability.
stream = true

# Whether to request JSON mode (response_format={"type":"json_object"}).
json_mode = true

[stream]
# Optional default stream for the session. If empty, the agent will pick a stream
# only when the user prompt explicitly mentions an existing stream name; otherwise
# it will ask the user to choose.
default = ""

[sink]
broker_url = "tcp://127.0.0.1:1883"
topic = "/nl2pipeline/out"
qos = 0

[repl]
check_max_attempts = 3
